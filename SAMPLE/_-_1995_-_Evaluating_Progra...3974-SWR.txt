Neighborhood Characteristics R23, Population, Project Evaluation, Regional Labor Markets, Social Discount Rate H43, Urban, Rural, Regional, Real Estate,  Transportation Economics: Regional MigrationEvaluating Program Evaluations: New Evidence  Commonly Used Nonexperimental Methods
 DANIEL FRIEDLANDER  PHILIP  . ROBINS*
Previous research  demonstrated  analysts  use social experiments  evaluate  likely reliability  nonexperimental program-evaluation methods (see Robert LaLonde, 1986,' Thomas Fraker  Rebecca Maynard, 1987; LaLonde  Maynard, 1987; James . Heckman  . Joseph Hotz, 1989).'  studies "evaluate" nonexperimental evaluation method  applying  method   data set produced  true random assignment
'Manpower Demonstration Research Corporation, 3 Park Avenue, New York, NY 10016,  Department  Economics, University  Miami, Coral Gables, FL 33124-6550, respectively.  research reported   paper  supported  grant   Alfred . Sloan Foundation.  findings  conclusions   necessarily represent  official position  policies   funding organization. Previous versions   paper  presented   1992 meetings   American Economic Association,  1991 meetings   Association  Public Policy  Management,   seminars   University  Wisconsin, Penn State University, Rutgers University,  University  North Carolina, Columbia University,  Urban Institute,   Alfred . Sloan Foundation.   indebted  George Cave, whose program created  statistically matched comparison groups used   study,   anonymous referee  provided detailed guidance  structuring  material presented   paper. Helpful comments   received  earlier versions   paper  George Cave, Barbara Goldman, Judith Gueron, James Heckman, . Joseph Hotz, Robert LaLonde, Robert Meyer, Robert Moffitt, Charles Romeo, Jeffrey Smith, Michael Wiseman,  seminar  ^orkshop participants,  members   MDRC Committee  Welfare Studies. Outstanding research assistance  provided  Dan Edelstein  Scott Susin. '  paper, experimental evaluation  defined   based  comparison  program group  control group,     created  randomly assigning sample members   group   . nonexperimental evaluation    uses comparison group  generated  random assignment  substitute  control group. 923
experiment  comparing  results   estimates produced   experimental method.  experimental estimates  presumed   unbiased estimates   true program effects. Significant differences    sets  estimates  taken  mean   econometric model generating  nonexperimental estimates  misspecified. Analysts  further use  experimental results  examine  standard specification tests   nonexperimental econometric model properly lead   reject specifications  yield estimates inconsistent   experimental findings (Orley Ashenfelter, 1978; Ashenfelter  David Card, 1985).   study,  extend  approach  assess  conventional nonexperimental strategies  estimating  effect  social programs.^   approach estimates  effects  policy change   area  comparing persons   area    jurisdiction  affected   policy change.^  example,  Long  Wissoker (1992), individuals  several counties   state  Washington  given new welfare-to-work program,   behavior  compared   behavior  individuals   counties 
 general discussion  nonexperimental strategies  evaluating social programs, see Robert Moffitt (1991). Robinson . Hollister  Jennifer Hill (1995) provide numerous examples  nonexperimental program evaluations  economics   disciplines.  comparisons   made across states (George Farkas ., 1983, 1984; Denise Poiit ., 1985), across counties ( cities)  states (Randall Brown ., 1983; Invin Garfmkel ., 1992; Sharon . Long  Douglas . Wissoker, 1982; Bradley . Schiller  . Nielsen Brasher, 1993),  across areas  cities (David . Long, 1991).
924

SEPTEMBER 1995
 new program   available.   approach compares  behavior  persons  particular area covered   policy change   behavior  individuals   same area   change went  effect.  example,  Paul . Decker (1991), welfare recipients  New Jersey  given access  new program  employment  training services,   behavior  compared   behavior  welfare recipients  period prior   implementation   new program.  obvious shortcoming   conventional nonexperimental evaluation strategies   inherent difficulty  controlling  differences  local conditions   program  comparison sites   changes   conditions  time. Nonetheless,   important  evaluate  comparison strategies ,  practice, variants    often   ones available  analyst. Data limitations, opposition  social experiments,   nature  certain programs  "entitlement"  prevent analysts  randomly assigning some persons   program locality  noprogram control group.   paper,  follow previous research  using experimental data  assess   nonexperimental evaluation approaches.  data   series  social experiments conducted  several states   198O'  evaluate programs aimed  helping welfare recipients find jobs.  simulate   nonexperimental approaches  creating comparison groups   true control groups   comparing  resultant nonexperimental estimates  program effects  experimentally derived estimates  program effects. Although experimental data  required  provide assessment   nonexperimental approaches,    required  create  comparison groups  practice. ,  results  direct implications  choosing nonexperimental evaluation strategy  experiment   feasible;  remainder   paper  organized  follows. Section  describes  social experiments   various comparison groups created   analysis. Section 
discusses  methods used  generate  assess  nonexperimental estimates. Section  presents  empirical results,  Section  offers some conclusions. . Data   early 198O', number  states undertook changes   welfare-to-work programs.  aim   program changes   increase employment   decrease welfare receipt.  new programs  require welfare recipients,  condition  receiving  full amount   monthly welfare payment,  look  work  attend job-search assistance groups,  participate  job training,   work  unpaid, government-sponsored job."*  estimate  effects   new programs, several states relied  social experiments: welfare recipients eligible  program  randomly assigned   program group,   subject   program requirements, services,  penalties,   control group,   .   various goals,  experiments  designed  estimate effects   employment  welfare receipt   program group.  four experiments analyzed   paper   evaluations   Arkansas WORK Program,  Baltimore Options Program,  San Diego Saturation Work Initiative Model (SWIM),   Virginia Employment Services Program (ESP).^ 
Unpaid work assignments  usually part-time  limited   months.  four experiments   set  nine   stimulated  federal welfare reform legislation  1981.   selected    evaluations  large-scale programs    least  years  follow-up data. Summaries   research findings   nine experiments (plus some others)  presented  Judith . Gueron (1990), Gueron  Edward Pauly (1991), Friedlander  Gueron (1992),  Greenberg  Wiseman (1992). Details   four experiments used   study  given  Friedlander . (1985a, ), Gayle Hamilton  Friedlander (1989),  James Riccio . (1986).
. 85 . 4
FRIEDLANDER  ROBINS: EVALUATING PROGRAM EVALUATIONS
925
  experiments  initiated  three-year period, 1982-1985.   implemented  single-parent families (headed mostly  women)   receiving benefits   Aid  Families  Dependent Children (AFDC) program.  experimental program  mandatory   failure  participate  result  partial, temporary reduction  AFDC payments. Individuals  children under  age  six (  Arkansas)  exempted   participation requirement    part   research samples. Typically,  half   persons enrolled  program group actually participated  formal employment  training activity.  remainder found work, left AFDC  participating,  penalized  failure  participate,   some cases, remained  AFDC    reached   employment program  . Owing largely  differences   levels  participation   mix  activities,  programs' net costs  program group member varied  $118  Arkansas  $953  Baltimore. Local environments varied considerably, too. State monthly AFDC grant levels ranged frorn $140  Arkansas  $526  San Diego. Unemployment rates varied  6.6 percent  Virginia  11.0 percent  Arkansas.  experimental evaluations found   four programs increased employment,     four programs (Arkansas  San Diego) reduced welfare receipt.^   study,  focus  employment effects.^ 
define short-term employment  having  earnings   third quarter   experiment,    quarter   quarter  random assignment.  define long-term employment  having  earnings  quarters 6-9   experiment (  year  random assignment). . Construction  Comparison Groups   study,  constructed four comparison groups     welfare-towork programs.   utilizes  control group  another state (     states combined)  comparison group   program group   original state.^  program  comparison groups  similar      AFDC case heads  meet  same eligibility criteria  mandatory welfare-to-work program, although local labor-market conditions varied considerably across  states   procedures  bringing individuals   samples differed  some details.   comparison group  based  statistical matching.  observed background characteristics   program group member  matched against   every candidate   comparison group;  candidate  closely resembling  program group member  selected     comparison group (see .., Katherine . Dickinson ., 1984, 1987).   study,  use procedure    program group member  matched non-
^ complete set  experimental estimates  program effects  presented  unpublished appendix available   autbors upon request. ' analyze employment rather  earnings  eliminate  need  adjust  state differences  wage levels   cost  living.  simplify  exposition,    present results  welfare receipt.  earlier version   paper, available   autbors upon request,  report limited comparisons  experimental  nonexperimental estimates   welfare outcomes. Generally,  welfare estimates performed worse   employment estimates,    often produced conflicting results (.., nonexperimental welfare estimates 
accurate  nonexperimental employment estimates  ,  vice versa). * example,   case   Arkansas WORK program,  generate  separate nonexperimental estimates   program effect using  control group   Baltimore Options program,  San Diego SWIM program,   Virginia ESP program. fourth nonexperimental estimate  produced  creating composite comparison group consisting   control groups  Baltimore, San Diego,  Virginia combined.  compare  nonexperimental estimates produced  eacb  tbese four comparison groups    experimental estimate produced  Arkansas.
926

SEPTEMBER 1995
parametrically  "nearest neighbor"   control groups    states using Mahalanobis distance metric'    comparison groups  subject  confounding interstate differences.  third  fourth comparison groups  .  third  based  comparisons   areas  offices  state  locality.    feasible  use  approach  cases   new program ( major program innovation)   implemented earlier   place   another.  example,   case  welfare-to-work programs, advance implementation   planned    group  welfare offices   several  urban area. Pilot testing   sort  common, although  testing  usually undertaken  identify problems  program design rather   estimate program effects. Nonetheless, sample collected   nonprogram offices   used  comparison group  sample collected under  same procedures    same time period    program offices.   study,  simulate  cross-site strategy   Arkansas  San Diego samples.'"  split
 Mahalanobis distance  given 
    vector  characteristics  program group member ()  matched comparison group member (),     total sample covariance matrix.  data  sorted randomly,    ^ ^  chosen     smallest.  comparison-group observation   used again    matched,  results  vary depending    data  initially sorted.  general description  nearest-neighbor techniques, see , , Hand (1981),    noted    several variants   nearest-neighbor technique  well   methods  statistical matching.  matched comparison samples  created  program written  George Cave   Manpower Demonstration Research Corporation,  matches  made  set  16 observed baseline characteristics, including prior employment  welfare history, age, number  ages  children, education, type  welfare case (applicant  recipient), marital status,  ethnicity, '°The  local offices  San Diego SWIM    same city (Service Center  San Diego West),  Arkansas,   offices   different cities (Little
   samples   local office samples.   office,  use  program group  produce nonexperimental estimates   program effect using controls    office  comparison group.  estimates  then compared   experimental estimates computed   program  control groups   same office.  fourth comparison group  based  before-after comparison  outcomes  particular area  office    utilized  new program  significant program change     implemented.  several months  year,  even  several years prior  implementation, sample  persons    eligible   program   drawn  serve  comparison group. Once  program begins,  same sampling procedures  continued,   new sample members constitute  program group.    cross-site strategy, state differences  eliminated   comparison, although cyclical labor-market differences  well  important,    events occurring   locality.  simulate  approach,  partition  samples roughly  half,  early  late cohort according  date  random assignment." Then,     four programs (..,   state), program group members   late cohort become  program group   analysis,  controls   early cohort
Rock  Pine Bluff),  Baltimore  Virginia,  presence   offices  small sample sizes   made  infeasible  perform one-against-one cross-site analysis.  further apart  time  cohorts ,  greater    risk  confounding environmental events,   longer    follow-up   early cohort  program start-up. Once  program begins,  follow-up   early cohort  end   program  delay working   early cohort members  still remain eligible   program.  practice, systematic  gradual phase-in  enrollment  participation, beginning   late cohort,  leave early cohort untouched  year  longer.
. 85 . 4
FRIEDLANDER  ROBINS: EVALUATING PROGRAM EVALUATIONS
927
become  comparison group.  compare  nonexperimental cross-cohort estimates   experimental estimates computed   program  control groups   late cohort   site. . Ttie Econometric Specification  nonexperimental estimates presented   paper  derived  linear regression model  adjusts  differences   characteristics  sample members observed just prior   entry   sample.'^ Several   characteristics   used  create  statistically matched cross-state comparison samples.'^  same regression specification  applied  produce experimental estimates  program effects. Although regression adjustment   strictly required  experimental estimates,  improves statistical precision somewhat  reducing residual variation  correcting  minor chance differences  exist   program  control groups  randomization.  dependent variables   regression model   short-term  long-term employment measures defined earlier.  independent variables include lagged employment (measured    quarter prior  random assignment),'"* vector  demographic characteristics   sample member prior  random assignment,  dummy variable equal  1   sample member   program group  0   sample member   comparison group
'^Heckman  Hotz (1989) found   linear regression model produced  best nonexperimental estimates  training effects  earnings  AFDC recipients. " some sense,  statistical-matching technique serves  same function   regression adjustment using observed preprogram personal characteristics. Indeed,    see,   cross-state methods perform   same despite  fact   matched comparison samples bear closer resemblance   program samples   basis   preprogram characteristics   unmatched comparison samples. "' Inclusion  additional periods  preprogram employment data  little impact   nonexperimental estimates.
(nonexperimental estimate)  control group (experimental estimate).  coefficient   program-group dummy   estimate   program' effect.  nonexperimental estimate,       unbiased estimate   true program effect, depending    error term  uncorrelated  correlated   program-group dummy.    preprogram values   outcome variable  demographic characteristics remove  correlation   error term   nonexperimental estimate  unbiased. Differences  motivation, labor markets, events,   factors  accounted    set  demographic variables  present   program group dummy   error term  induce bias.  experimental estimate   program effect  unbiased  randomization (assuming   implemented properly) ensures   error term  uncorrelated   program-group dummy.  experimental designs, "internal" validity  based   randomized assignment " sample members  program  control groups. Ensuring  validity  experiments generally rests  careful implementation   randomization process  continued monitoring   different treatment   research groups   follow-up period. Internal validity   subject  statistical testing, although   often "confirmed"   fact  verifying  observable pre-random-assignment demographics  similar across research groups.  nonexperimental designs, internal validity necessarily rests   validity   econometric specification   statistical methods used  estimate .  consequence,    argued  statistically testing  econometric model   integral part   nonexperimental estimation procedure.  model fails 
'^ argument  specification testing  variant   test  employ appeared   connection  employment  training program evaluations  Ashenfelter (1978).  rationale  developed further, along  additional tests,  Heckman  Hotz (1989).
928

SEPTEMBER 1995
fication test, then    declared  wrong model,     accept  estimates  program effects  produces.  model passes, then   grounds  accepting  validity   estimated program effects. According   argument,   quite possible   recent pessimistic appraisals  nonexperimental methods  estimating program effects  resulted largely  failure  apply specification tests (see Heckman  Hotz, 1989).  kind  specification test  concerned    econometric model  account   differences  outcomes  program  comparison groups except  induced   program. Operationally,  test generally looks    model "correctly" predicts  differences  outcomes   program  comparison groups   period   program group enters  program (..,   receive  "treatment').   study,  test   estimated program effect  different  zero   model  applied   preprogram period.     estimating linear regression model similar    used  estimate  experimental  nonexperimental program effects, except   dependent variable   individual' employment status    quarter prior  entry   sample (..,  program group members,   quarter  enrollment   program).  independent variables   same except   lagged value   dependent variable  measured  quarters prior   dependent variable.^"  specification test  based   coefficient   program-group dummy.   coefficient  significantly different  zero, then  nonexperimental estimator fails  test;
otherwise  passes.'^  use 10 percent   critical level  determining statistical significance.'^  test   advantage   readily understandable  quite easy  apply  practice. Specification testing   serious shortcoming, .  fit   model   preprogram period   necessary logical connection   validity   model   follow-up period. Becoming eligible  employment  training program frequently occurs  life transition,   dissolution  family group, loss  support, long stint  joblessness, entry  single mother' youngest child  school,   like. empirical specification  adequately describes behavior   transition,  therefore passes  specification test,   provide adequate control function  outcomes following  transition.   procedure  work well  short-term follow-up    long-term follow-up.   reason,  examine outcomes measured  short  long lengths  time   program begins.   find, through studies like  present ,   specification test generally leads  nonexperimental estimates   similar  experimental estimates  outcomes measured  different points  time  enrollment, then    confidence  using  procedure,   through repeated validation—not   theory indicates   test necessarily  work.   quite possible  empirical studies  show certain kinds  nonexperimental results   generally invalid even   pass specification test. Moreover,  test  work well   some groups  under some conditions.
Ideally, more preprogram data   used  specification testing.  example, Heckman  Hotz (1989)  others  argued  using long preprogram period   specification tests.  ability   testing   scale  limited   amount  preprogram data   ( year).
 more conservative test  posit     coefficients,    program group dummy,   same  preprogram  follow-up equations. Clearly, number  more complicated specifications  more complex testing procedures    utilize   adopted.   tested larger critical level  found   results  insensitive   level chosen.
. 85 . 4
FRIEDLANDER  ROBINS: EVAL UA TING PROGRAM EVAL UA TIONS
929
. Empirical Results large number  nonexperimental estimates  produced  comparison   experimental estimates.   crossstate comparisons,   120 pairs  experimental  nonexperimental estimates (96 unmatched  24 matched).^'   within-state comparisons,   40 pairs  experimental  nonexperimental estimates (16 across sites  24 across cohorts).^"  facilitate interpretation   results,   present detailed estimates  selected subset   experimental  nonexperimental pairs.  then present summary   results   160 pairs  estimates.  1 presents number  selected experimental  nonexperimental estimates  longer-term employment effects. ^^  cell     devoted   estimate  shows  following information (  top down): ()  estimated program effect, ()  standard error   estimated program effect, ()  probability value ( value)   specification test.
  unmatched analyses, equations  estimated     four states times four comparison groups  state (    states plus  combined   states) times  dependent variables (short-term  long-term employment) times  sample subgroups (short-term recipients  welfare, long-term recipients  welfare,   groups combined).   matched analyses, equations  estimated     four states times  comparison group  state times  dependent variables times  sample subgroups. ^"  analyses across sites, equations  estimated   welfare offices times  dependent variables times  sample subgroups  San Diego plus  welfare offices times  dependent variables  Arkansas (sample subgroup analyses   possible  Arkansas  sample sizes  too small).   analyses across cohorts, equations  estimated  four states times  dependent variables times  sample subgroups. ^'  cross-state/unmatched column,  pair   largest comparison group  shown   state ( pair   controls     states combined  used).   cross-state/matched column,  pairs  shown.   cross-site column,  pair   state  selected  random.   cross-cohort column,  pairs  shown.
 ()  sample size. specification test fails   value    0.10.  result indicates   program-group dummy  different  zero   dependent variable  employment    quarter prior  program entry.   taken  mean   comparison group   provide valid representation   preprogram behavior   particular program group  question    regression adjustment   succeeded  making  comparison valid.   four columns   1 provide information   cross-state estimates.     columns,  sets  experimental estimates  given, unadjusted  regression-adjusted. Comparing   provides check   implementation   experimental design.  randomization  properly implemented, then   cases regression adjustment  affect   standard errors    numerical estimate   program effect. ^^   1,  unadjusted  regression-adjusted experimental estimates   program effect  quite close      four programs,       experimental comparisons pass  specification test.   test failure occurs  Baltimore,   unadjusted experimental estimate   statistically significant    value   specification test    0.10.^^     experimental comparison  fail  specification test   1. Regression adjustment  observed baseline characteristics   experiment slightly increases  experimental estimate  program effect  makes  statistically significant. Regression adjustment  changes
True randomization wili occasionally create program  control groups ,  chance,  background characteristics  differ  statistically significant degree. control group created  wellimplemented random assignment procedure, therefore, wJJJ occasionaiJy fail  specification test.  coefficient   program-group dummy   baseline period  significantly negative, suggesting   unadjusted experimental estimate  biased downward.
930

SEPTEMBER 1995
 1—SELECTED ESTIMATES  PROGRAM EFFECT  FRACTION EMPLOYED 6-9 QUARTERS  BASELINE Cross-state estimates Experiment! estimates Program Arkansas, WORK Unadjusted 0,050* (0,027) [0,119J Af-1,127 Baltimore, OPTIONS 0,030 (0,019) 10,064]  =. 2,757 San Diego, SWIM 0,091" (0,017) [0,773] /V-3,211 Virginia, ESP 0,050" (0,019) [0,440] Af-3,150 Adjusted 0,057* (0,025) [0,230] = 1,127 0,041* (0,018) [0,191 ]  " 2,757 0,090" (0,017) [0,951 ] /-3,211 0,060** (0,018) [0,891 ] JV-3,150 Nonexperimental estimates Unmatehed -0,139** (0,024) [0,000]   4,593 0,132** (0,015) [0,165]   4,567 0,020 (0,016) [0,883 ]   4,597 0,115** (0,013) [0,017]  = 5,688 Matched -0,152** (0,028) [0,896] JV-1,120 0,127** (0,018) [0,681]   2,724 0,034* (0,017) [0,285 ]   3,208 0,127** (0,014) [0,230]   4,238 " 0,078** (0,024) [0,678]   1,603 _ 0,123** (0,024) [0,201 ] Af-1,620 Across site Experimental 0,064* (0,032) [0,523] W-692 Nonexperimental 0,101** (0,038) [0,849] '-543 Within-state estimates Across cohort Experimental 0,009 (0,034) [0,956] '-605 0,041* (0,023) [0,140]   1,725 0,083** (0,026) [0,709]   1,453 , 0,058* (0,024) [0,492]   1,673 Nonexperimental 0,067 * (0,035 ) [0,600]
N-sia
0,081** (0,026) [0,280] '  1,380 0,067** (0,024) [0,303]   1,620 0,090** (0,026) [0,000] '  1,620
Notes:  top  bottom,  entries   cell  () estimate  program effect, () standard error  estimated program effect ( parentheses), ()  value  specification test ( brackets),  () sample size.  tested  difference   nonexperimental estimate    experimental estimate   left       statistical significance.  differences  found   statistically significant   unmatched  matched cross-state estimates   none   cross-site  cross-cohort estimates, 'Virginia, unlike   experiments,  program-group:control-group size ratio  2 : 1 ; matching therefore yields sample larger   original evaluation sample, ••Statistically significant   10-percent level, •Statistically significant   5-percent level, *'Statistically significant   1-percent level.
 specification-test statistic  "pass," indicating  regression  controlled   differences  baseline characteristics.    interest  note   failed (unadjusted) estimate  program effect  quite close   passed (regression-adjusted) estimate  program effect.  ,  test   seem   serving  function  eliminating  inaccurate estimates  keeping    close   best estimate.  1 next shows unmatched  matched nonexperimental cross-state estimates.  estimates use  program group   state  comparison group created   control groups     states.  unmatched  matched cross-state nonexperimental estimates  similar     differ considerably
  regression-adjusted experimental estimates (   column).  Arkansas,    nonexperimental estimates    wrong sign   differ   0.20.    states,  signs   experimental  nonexperimental estimates   same,   magnitudes  never closer   0.05. Statistical inferences differ (..,   estimate  program effect  pair  statistically significant   10-percent level    statistically significant   opposite signs)     eight pairs  experimental  nonexperimental estimates.    differences   cross-state nonexperimental estimates ( unmatched  matched)   corresponding regression-adjusted experimental estimates  statistically significant.
. 85 . 4
FRIEDLANDER  ROBINS: EVALUATING PROGRAM EVALUATIONS
931
program group   same locality.   results suggest  using individuthree   four pairs,  differences beals   state  comparison group  tween experimental  nonexperimental individuals  another state  lead  quite estimates    0.05,   statistiinaccurate estimates   size  procal inferences   same.  none   gram effect, even    groups  cohort pairs   experimental  nonmatched statistically according  set  experimental estimates significantly differbaseline characteristics. Moreover,  specent.  specification test unfortunately ification test  difficulty discriminating rejects    nonexperimental estibetween accurate  inaccurate nonexperimates (Virginia).   fails  reject  mental estimates.     unArkansas nonexperimental estimate,  matched nonexperimental estimates  differs more   paired experimental none   matched nonexperimental estiestimate      cross-cohort mates  rejected   specification test. estimates   yields different statistiMoreover,   Arkansas  Virginia, cal inference.  specification test rejects  unmatched  2 summarizes  results   160 estimate  accepts matched estimate pairs  experimental  nonexperimental   even further   experimental estimates (including    1).   estimate.  1,  column represents different  situation improves somewhat  comparison-group specification.   switching  cross-state  within-state  panels   :  top    comparisons,  shown   right side   1 (last four columns). , experi- pairs  experimental  nonexperimental estimates,  middle   pairs   mental estimate  shown   local ofthe nonexperimental comparison passes  fice  Arkansas    San Diego. Next specification test,   bottom    shown nonexperimental estimate propairs    nonexperimental comparduced  using  comparison group  ison fails  specification test.   row control group    local office     panel gives  number  pairs  evaluation sample.^"   pairs,  exexperimental  nonexperimental estiperimental  nonexperimental estimates mates   panel.   row     0.05   ,  differpanel gives  mean   experimental ences   statistically significant,  estimates   panel.  third row   statistical inferences   same.   panel gives  mean   absolute nonexperimental estimates pass  specifidifference   experimental   test. nonexperimental estimates   pair,   last  columns   1 show  principal summary measure."  fourth  kind  within-state estimates, row   panel gives  percentage  namely,  cross-cohort estimates.  pairs    experimental  nonexnext-to-last column shows  experimental perimental estimates   program effect estimates   program effect created  yield different statistical inferences (based using program  control group members   10-percent level  significance). Firandomly assigned  late cohort (.., late nally,  fifth row   panel gives    period  random assignment).  last column shows nonexperimental estimates using  early-cohort control group  comparison group   late-cohort
^ average absolute difference  presented,   average difference  .  latter   meaningful   cross-state estimates (particularly  unmatched sample)    cross-site estimates,   control group   state  site serves  comparison group  nonexperimental estimate, forcing  average difference   close  zero  definition.
^" San Diego, Service Center   program office  San Diego West   comparison office.  Arkansas, Little Rock   program site  Pine Bluff   comparison site.
932
 AMERICAN ECONOMIC
REVIEW
SEPTEMBER 1995
 2—SUMMARY  EXPERIMENTAL  NONEXPERIMENTAL ESTIMATES  PROGRAM EFFECT  FRACTION EMPLOYED
Statistic
Comparison group specification Cross-state; estimates Within-state estimates Unmatched Matched Across site ,Across cohort
 Pairs  Experimental  Nonexperimental Estimates: Number  pairs 96 24 Mean experimental estimate 0,056 0,056 Mean absolute experimental 0,090 0,080 nonexperimental difference Percentage  different 47 38 inference Percentage  statistically 70 67 significant difference (10-percent level) Pairs  Pass Specification Test: Number  pairs Mean experimental estimate Mean absolute experimental — nonexperimental difference Percentage  different inference Percentage  statistically significant difference (10-percent level) Pairs  Eail Specification Test: Number  pairs Mean experimental estimate Mean absolute experimental nonexperimental difference Percentage  different inference Percentage  statistically significant difference (10-percent level)
16
0,069 0,044
24
0,045 0,034
13 31
29 4
32
0,052 0,059
18
0,048 0,080
14
0,057 0,042
16
0,046 0,030
41 56
39 72
14 29
19 0
64
0,057 0,105
6
0,077 0,080
2
0,157 0,063
8
0,043 0,042
50 11
33 50
0 50
50 13
percentage  pairs    difference  experimental  nonexperimental estimates  statistically significant   10-percent level.^*
  important  recognize   sample sizes vary across  comparison-group specifications.  particular, sample sizes  smaller   within-state specifications    cross-state specifications.  comparing  results across  different comparisongroup specifications, potential biases related  sample size   kept  mind.    criterion used  judge  accuracy   nonexperimental estimates, mean absolute difference, decreasing sample size  tend  produce worse results, assuming     change   real underlying bias.    criteria  tend  show (falsely) improvement  sample size decreases.  given size  
 top panel   2 displays results   pairs  experimental  nonexperimental estimates, ignoring   nonexperimental part   pair passes  fails  specification test. Looking    cross-state estimates    utilize statistical matching (  column  results),   seen   mean absolute difference
program effect, "different inference"  occur  often  smaller samples    more likely   experimental  nonexperimental estimates    statistically significant. Similarly, fewer statistically significant differences  occur  smaller samples,   bias   nonexperimental estimate,  matter  slight,  yield "difference statistically significant"    samples  large enough.
. 85 . 4
FRIEDLANDER  ROBINS: EVALUATING PROGRAM EVALUATIONS
933
 large,  larger   mean experimental estimate itself. Nearly half   nonexperimental estimates (47 percent) lead  different statistical inferences     corresponding experimental estimates. even larger share   pairwise differences (70 percent)  statistically significant.  third   pairs differ  more  0.100 ( shown   ),  indicates substantial discrepancy, given   average   experimental estimates + 0.056.  third differ    0.050 ( shown). Statistical matching improves  accuracy   cross-state nonexperimental estimates,     .  mean absolute experimental-nonexperimental difference falls  0.080 ( 11-percent drop),    still larger   mean experimental estimate.  percentage   estimates producing different inference falls  38 percent ( 19percent drop),   percentage  statistically significant difference falls  67 percent ( 4-percent drop). , even though  program  comparison groups  more closely matched   observed baseline personal characteristics,  greater similarity translates   modest rather  substantial improvement   accuracy   nonexperimental estimates.   several reasons why statistical matching   more dramatically improve  accuracy   nonexperimental estimates. , matching utilizes  observed characteristics, measured  baseline.    substantial unobserved differences   program  comparison groups    differences influence employment decisions independently  observed characteristics, then matching  likely   little effect   comparability    groups.  unobserved differences   environment-related (.., differences  economic conditions  local institutional structure)     individual-related (.., differences  community preferences  work). ,  particular matching procedure used     given enough weight    important baseline characteristics. Some  matching procedure might prove
superior. Third,  matching variables   included  regressors   models  program effects    matched  unmatched samples.   unmatched samples,  regressors perform   same function  matching (..,  adjust  outcome variable  differences   observed characteristics).   matched samples,  characteristics  used twice:   creating  matched sample,  then again  adjusting  outcome variable   regression model.  fact,   nonexperimental models  estimated without using  characteristics  regressors,  matched-sample results hardly change  , whereas  unmatchedsample results change more  differ more     experimental counterparts. Finally,    noted  matching  preprogram characteristics   logically imply   model relating  characteristics  subsequent behavior  necessarily   same   matched comparison group   program group   absence   program.  fact,  crux   nonexperimental evaluation problem   individuals  identical measured baseline characteristics  exhibit divergent follow-up outcomes. Looking next   last  columns   2,   evident   within-state comparisons perform considerably better   cross-state comparisons,  inaccuracies still remain. Mean absolute differences   smaller       mean experimental estimates.  decrease  mean absolute differences  particularly notable   smaller sample sizes  tend  produce increases   measure (see footnote 26). Mean absolute differences, ,  still large enough   worrisome (64 percent   mean experimental estimate   cross-site specification  76 percent   mean experimental estimate   cross-cohort specification). Note   13 percent   cross-site pairs   29 percent   cross-cohort pairs produce different inferences.  note   31 percent   cross-site pairs  4 percent   cross-cohort pairs produce statistically significant differences.  latter  criteria, , might tend  indicate
934

SEPTEMBER 1995
fewer significant differences anyway   cross-state samples  divided   smaller within-state subsamples (see footnote 26).   middle  bottom panels   2,  pairs  grouped separately  nonexperimental comparisons  pass  fail  specification test, respectively.   specification test  useful,    results   middle panel   accurate.     desirable,   strictly necessary,   bottom panel contained mostly inaccurate estimates,    mean   test  waste fewer accurate estimates.  results indicate   specification test  improve  accuracy   nonexperimental estimates.   columns   ,  mean absolute difference  lower   group  pairs  pass  specification test      pairs. ,  ability   specification test  discriminate  accurate  inaccurate estimates   great   samples,   improvement  using  test  often marginal.   cross-state/unmatched comparisons, two-thirds   nonexperimental estimates fail  specification test.  high failure rate   necessarily bad.  fact,   hope  large proportion  test failures,      results   far off  mark.   turns , weeding   test failures  improve results somewhat.  mean absolute difference drops  third ( 0.090  0.059),    smaller proportions  different inferences  statistically significant differences.  failed cross-state/unmatched estimates  mostly  pairs  statistically significant experimental-nonexperimental differences (77 percent) ,  lesser extent, different statistical inference (50 percent).  half  failed pairs differ  more  0.100 ( shown   ). Indeed,  specification test seems  effective  eliminating  "outlier" estimates (.., nonexperimental estimates differing  more  0.100   corresponding experimental estimates).  ,   54 cross-state/unmatched pairs
 differences  niore  0.100,  47 (more  85 percent)  identified  failing  specification test. Notwithstanding,    crossstate/unmatched nonexperimental estimates  pass  specification test still differ substantially   respective experimental estimates: 41 percent   "passes" yield different statistical inferences,  56 percent   differences  statistically significant.  cross-state/matched estimates   show improvement   specification test,  did  unmatched estimates.  one-quarter   matched estimates fail  test.  passed estimates   same mean absolute difference   set   estimates   column;   absence   kind  improvement observed   unmatched estimates. More   passed estimates  different inference,  more  statistically significant difference.  within-state estimates show  marginally greater benefit   specification test   cross-state/matched estimates.  great majority   withinstate estimates pass  specification test.   some improvement  cross-site  cross-cohort comparisons,   set  passes   markedly better   set   untested estimates. . Conclusions  social programs  evaluated using nonexperimental econometric methods  compare  behavior  individuals exposed   program   behavior  individuals   .   paper   followed previous studies  using experimental data  assess  relative efficacy  certain types  nonexperimental procedures   frequently used  evaluate social programs.    procedures face  common problem  selecting appropriate comparison group.   examined  statistical techniques  improving accuracy: statistical matching  specification testing.  procedures   utilized   paper   meant  represent  full catalogue  applicable
. 85 . 4
FRIEDLANDER  ROBINS: EVALUATING PROGRAM EVALUATIONS
935
nonexperimental comparison methods, matching techniques,  specification tests.   set  nonexperimental estimates utilized comparison samples drawn  different states   program samples. Nonexperimental evaluations often adopt  method.  resulting nonexperimental estimates  usually quite different   experimental estimates derived   same data.    surprising,  program  comparison samples   cross-state procedures  far apart  space  time,  environmental factors  differ considerably. More importantly, ,  found  specification testing  statistical matching procedures did  appear  make major improvements   cross-state estimates.  specification test did reject    nonexperimental estimates,    remained   somewhat better,  markedly better,   original untested set  estimates. Statistical matching  produced  modest improvement   accuracy   nonexperimental estimates.   set  nonexperimental estimates utilized comparison samples drawn   same state   program sample.  approach produced better results   cross-state procedures.  average discrepancy  experimental  nonexperimental estimates  smaller, although important differences still remained. Furthermore, applying specification test did  improve  within-state estimates further.^' Overall,  specification test  more effective  eliminating wildly inaccurate "outlier" estimates   pinpointing   accurate nonexperimental estimates. Making  comparison samples closer  time  space  using cross-site  cross-cohort comparisons produced more improvement  specification testing  statistical matching.  results   study illustrate  risks involved  comparing  behavior 
individuals residing   different geographic areas. Comparisons across state lines  particularly problematic.  findings illustrate  estimates  program effects  cross-state comparisons   quite far   true effects, even  samples  drawn (  )   same sample intake procedures   target populations defined   same objective characteristics.   important lesson, given  studies often rely  cross-state comparisons.  results suggest  statistical matching  specification test alone   unable  reduce markedly  uncertainty surrounding  kind  nonexperimental estimate.  research indicates ,  minimum,  studies  demonstrate  similarity  local conditions  prerequisite  establishing  validity   comparison.  similarity  observable local conditions  sufficient  valid comparison   areas remains open question, .   switched  comparison  across states   state  did note some improvement,  inaccuracies still remained. Additional research  ask   inaccuracies   reduced further  larger sample sizes,  using particular local variables  identify suitable comparison area (  verifying later   area remained suitable throughout  study period),   pooling estimates  program effects  number  independent comparisons  comparison studies. REFERENCES Ashenfelter, Orley. "Estimating  Effect  Training Programs  Earnings." Review  Economics  Statistics, February 1978, 60(1), . 47-57. Ashenfelter, Orley  Card, David. "Using  Longitudinal Structure  Earnings  Estimate  Effect  Training Programs." Review  Economics  Statistics, November 1985, 67(4), . 648-60, Brown, Randall; Burghardt, John; Cavin, Edward; Long, David; Mallar, Charles; Maynard, Rebecca; Metcalf, Charles; Thornton, Craig  Whitebread, Christine. Final report: Employment opportunity pilot project:
Additional analysis  reported   paper indicates   findings   sensitive   choice  dependent variables,   subgroup analyzed,    size   critical region   specification test.
936

SEPTEMBER 1995
Analysis ofprogram impacts. Princeton, NJ: Mathematica Policy Research, February 1983. Decker, Paul . Estimating  effects   REACH program  AFDC receipt. Princeton, NJ: Mathematica Policy Research, August 1991. Dickinson, Katherine .; Johnson, Terry .  West, Richard . " Analysis   Impact  CETA Programs  Participants' Earnings." Final report prepared   .. Department  Labor, Employment  Training Administration, SRI International, Menlo Park, CA, November 1984. . " Analysis   Sensitivity  Quasi-Experimental Net Impact Estimates  CETA Programs." Evaluation Review, August 1987, 7/(4), . 452-72. Farkas, George; Smith, David  Stromsdorfer, Ernst. " Youth Entitlement Demonstration: Subsidized Employment  Schooling Requirement." Journal  Human Resources, Fall 1983, 75(4), . 557-73. Farkas, George; Olsen, Randall; Stromsdorfer, Ernst .; Sharpe, Linda ; Skidmore, Felicity; Smith, . Alton  Merrill, Sally. Post-program impacts   youth incentive entitlement pilot projects. New York: Manpower Demonstration Research Corporation, June 1984. Fraker, Thomas  Maynard, Rebecca. "Evaluating Comparison Group Designs  Employment-Related Programs." Journal  Human Resources, Spring 1987, 22(2), . 194-227. Friedlander, Daniel  Gueron, Judith . " High-Cost Services More Effective  Low-Cost Services?"  Charles . Manski  Irwin Garfinkel, eds.. Evaluating welfare  training programs. Cambridge, MA: Harvard University Press, 1992, . 143-98. Friedlander, Daniel; Hoerz, Gregory; Long, David  Quint, Janet. Maryland: Final report   employment initiatives evaluation. New York: Manpower Demonstration Research Corporation, December 1985a. Friedlander, Daniel; Hoerz, Gregory; Quint, Janet  Riccio, James. Arkansas: Final report   work program   counties.
New York: Manpower Demonstration Research Corporation, September 1985b. Garfinkel, Invin; McLanahan, Sara  Robins, Philip . Child support assurance: Design issues, expected impacts,  political barriers  seen  Wisconsin. Washington, DC: Urban Institute Press, 1992. Greenberg, David .  Wiseman, Michael. " Did  OBRA Demonstrations ?"  Charles . Manski  Irwin Garfinkel, eds.. Evaluating welfare  training programs. Cambridge, MA: Harvard University Press, 1992, . 25-75. Gueron, Judith . "Work  Welfare: Lessons  Employment Programs." Journal  Economic Perspectives, January 1990, 4(1), . 79-98. Gueron, Judith .  Pauly, Edward.  welfare  work. New York: Russell Sage Foundation, 1991. Hamilton, Gayle  Friedlander, Daniel. Final report   saturation work initiative model  San Diego. New York: Manpower Demonstration Research Corporation, November 1989. Hand, . . Discrimination  classification. New York: Wiley, 1981. Heckman, James .  Hotz, . Joseph. "Choosing  Alternative Nonexperimental Methods  Estimating  Impact  Social Programs:  Case  Manpower Training." Journal   American Statistical Association, December 1989, 54(408), . 862-74. Hollister, Robinson .  Hill, Jennifer. "Problems   Evaluation  Community-Wide Initiatives,"  James . Connell, Anne . Kubisch, Lisbeth . Schorr,  Carol . Weiss, eds.. New approaches  evaluating community initiatives: Concepts, methods,  contexts. Washington, DC: Aspen Institute, 1995, . 127-72. LaLonde, Robert. "Evaluating  Econometric Evaluations  Training Programs  Experimental Data." American Economic Review, September 1986, 76(4), . 604-20. LaLonde, Robert  Maynard, Rebecca. " Precise  Evaluations  Employment  Training Programs: Evidence  Field Experiment." Evaluation Review,
. 85 . 4
FRIEDLANDER  ROBINS: EVALUATING PROGRAM EVALUATIONS
937
August 1987, 11(4), . 428-51. Long, David . "Cleveland Analysis Plan." Mimeo, Manpower Demonstration Research Corporation, New York, December 1991. Long, Sharon ,  Wissoker, Douglas . Net impacts   Washington State Family Independence Program (FIP):    years. Washington, DC: Urban Institute, June 1992. Moffitt, Robert. "Program Evaluation  Nonexperimental Data." Evaluation Review, June 1991, 15(3), . 291-314.
Polit, Denise; Kahn, Janet  Stevens, David. Final impacts  project redirection. New York: Manpower Demonstration Research Corporation, 1985. Riccio, James; Cave, George; Freedman, Stephen  Price, Marilyn. Final report   Virginia Employment Services Program. New York: Manpower Demonstration Research Corporation, August 1986. Schiller, Bradley .  Brasher, . Nielsen. "Effects  Workfare Saturation  AFDC Caseloads." Contemporary Policy Issues, April 1993, 11(2), . 39-49.
 